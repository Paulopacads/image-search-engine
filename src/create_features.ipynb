{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to use for PyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset that loads the images from a folder\n",
    "class FolderDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.transform = transform\n",
    "        self.files = os.listdir(folder_path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.folder_path, self.files[idx])\n",
    "        image = Image.open(img_path)\n",
    "        # Convert the image to a PyTorch tensor\n",
    "        image = transforms.ToTensor()(image)\n",
    "        # resize the image to 224x224\n",
    "        image = transforms.Resize((224, 224))(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset that loads the images from the \"images\" folder\n",
    "image_dir = \"static/data/jpg\"\n",
    "image_list = os.listdir(image_dir)\n",
    "dataset = FolderDataset(image_dir)\n",
    "\n",
    "# Create a dataloader for the dataset\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=1, shuffle=False, num_workers=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/temsfrog/anaconda3/envs/pfee-smith/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/temsfrog/anaconda3/envs/pfee-smith/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained VGG19 model\n",
    "model = torchvision.models.densenet121(pretrained=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move the model to the specified device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = ['image_id', \"features\"]\n",
    "# create csv file\n",
    "f_features = open('data/features_densenet_test.csv', 'w')\n",
    "# initialize writer for csv\n",
    "writer_features = csv.writer(f_features)\n",
    "# write header\n",
    "writer_features.writerow(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1491/1491 [01:47<00:00, 13.84it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "# Extract features from the images in the dataset\n",
    "for i, inputs in enumerate(tqdm.tqdm(dataloader)):\n",
    "    # Move the input images to the specified device\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Extract the features from the intermediate layer of the VGG19 model\n",
    "    features = model.features(inputs)\n",
    "\n",
    "    # Convert the features to a NumPy array\n",
    "    features = features.detach().cpu().numpy()\n",
    "    # Reshape the features to a 1D array\n",
    "    features = features.reshape(features.shape[0], -1)\n",
    "    # to string\n",
    "    features = features[0].tolist()\n",
    "    \n",
    "    # write to csv\n",
    "    writer_features.writerow([image_list[i], features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the file\n",
    "f_features.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pfee-smith')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02bd647a45f43a10576129fea97fd705bcec0a3de0d160e44fc4de987aa943e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
